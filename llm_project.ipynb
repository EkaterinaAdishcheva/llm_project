{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4377bac9-69d4-436c-b53f-b735ec9982c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running"
     ]
    }
   ],
   "source": [
    "!curl http://127.0.0.1:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3067c8d-7501-47d9-8b3d-2368ba47aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "MODEL_NAME = \"mistral\"  # Change to \"phi3\" or \"gemma:2b\" if needed\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = OllamaLLM(\n",
    "    model=MODEL_NAME,\n",
    "    keep_alive=-1,\n",
    "    format=\"json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e455038c-cc21-4de1-9062-1a4259f8eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa881d2d-50f2-4da1-a432-87593c832b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response_simple(request_str):\n",
    "    system_prompt = SYSTEM_PROMPT\n",
    "    \n",
    "    # Create a properly formatted prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        # (\"system\", system_prompt),\n",
    "        (\"human\", request_str)\n",
    "    ])\n",
    "    \n",
    "    # Format the final prompt before passing it to `llm.invoke()`\n",
    "    formatted_prompt = prompt.format_messages()\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Print the response\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ed49ab1-692f-4c78-bf6f-c5dae2646178",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [\n",
    "    \"Какой рецепт вегетарианской пасты?\",\n",
    "    \"Как приготовить блинчики?\",\n",
    "    \"Что можно приготовить из куриного филе?\",\n",
    "    \"Как сделать шоколадный торт?\",\n",
    "    \"Как приготовить рис для суши?\",\n",
    "    \"Как приготовить суп-пюре из тыквы?\",\n",
    "    \"Что можно приготовить из картошки и сыра?\",\n",
    "    \"Как сделать домашний хлеб?\",\n",
    "    \"Какие десерты можно приготовить без муки?\",\n",
    "    \"Как приготовить рис с курицей и овощами?\",\n",
    "    \"Как сделать соус для пасты?\",\n",
    "    \"Как приготовить морковные котлеты?\",\n",
    "    \"Что приготовить на ужин за 30 минут?\",\n",
    "    \"Как сделать домашнюю пиццу?\",\n",
    "    \"Что можно приготовить из куриного филе, риса и брокколи?\",\n",
    "    \"Как приготовить сладкий омлет?\",\n",
    "    \"Как сделать домашнее мороженое без мороженицы?\",\n",
    "    \"Что можно приготовить для обеда, если нет мяса?\",\n",
    "    \"Как приготовить крем-суп из грибов?\",\n",
    "    \"Как приготовить рыбу с картошкой в духовке?\",\n",
    "    \"Что приготовить для завтрака, если есть яйца, авокадо и помидоры?\",\n",
    "    \"Как приготовить куриные крылышки на гриле?\",\n",
    "    \"Что приготовить на праздничный ужин?\",\n",
    "    \"Как приготовить соус бешамель?\",\n",
    "    \"Как сделать торт без яиц?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6f1077b-d29f-4e8f-b451-a96bc1988148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:45<00:00,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for req in tqdm(requests):\n",
    "    start = time.time()\n",
    "    res = llm_response(req)\n",
    "    responses.append({\n",
    "        \"request\": req,\n",
    "        \"time\": time.time() - start,\n",
    "        \"reqponse\": res,\n",
    "        \"desc\": \"llm_response_simple\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea4914e2-edc0-4485-a6de-e95b2ca39222",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_SIMPLE = \"Ты — кулинарный помощник\"\n",
    "SYSTEM_PROMPT = \"\"\"Ты — кулинарный помощник, который рекомендует рецепты на основе запросов пользователей. \"\n",
    "            \"Используй доступные ингредиенты и предпочтения пользователя, чтобы предложить лучший рецепт. \"\n",
    "            \"Если у пользователя нет конкретных предпочтений, предложи что-то популярное или сезонное. \"\n",
    "            \"Опиши рецепт кратко: укажи название, основные ингредиенты и способ приготовления. \"\n",
    "            \"Если возможно, укажи калорийность и полезные свойства блюда. \"\n",
    "            \"Отвечай только на русском языке.\"\"\"\n",
    "\n",
    "def llm_response_with_prompt(request_str, system_prompt):\n",
    "    \n",
    "    # Create a properly formatted prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", request_str)\n",
    "    ])\n",
    "    \n",
    "    # Format the final prompt before passing it to `llm.invoke()`\n",
    "    formatted_prompt = prompt.format_messages()\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Print the response\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8430aef-1b48-45d0-a1fc-3cba565f81cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:40<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "responses_simple_prompt = []\n",
    "for req in tqdm(requests):\n",
    "    start = time.time()\n",
    "    res = llm_response_with_prompt(req, SYSTEM_PROMPT_SIMPLE)\n",
    "    responses_simple_prompt.append({\n",
    "        \"request\": req,\n",
    "        \"time\": time.time() - start,\n",
    "        \"reqponse\": res,\n",
    "        \"desc\": \"llm_response_with_prompt(req, SYSTEM_PROMPT_SIMPLE)\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c87e7b9-d90e-4398-b92c-28bc6061d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:53<00:00,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "responses_prompt = []\n",
    "for req in tqdm(requests):\n",
    "    start = time.time()\n",
    "    res = llm_response_with_prompt(req, SYSTEM_PROMPT)\n",
    "    responses_prompt.append({\n",
    "        \"request\": req,\n",
    "        \"time\": time.time() - start,\n",
    "        \"reqponse\": res,\n",
    "        \"desc\": \"llm_response_with_prompt(req, SYSTEM_PROMPT)\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a484f-2473-4dcb-94ce-c51d6648a8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_venv)",
   "language": "python",
   "name": "llm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
