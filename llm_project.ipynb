{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377bac9-69d4-436c-b53f-b735ec9982c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://127.0.0.1:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3067c8d-7501-47d9-8b3d-2368ba47aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "MODEL_NAME = \"mistral\"  # Change to \"phi3\" or \"gemma:2b\" if needed\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = OllamaLLM(\n",
    "    model=MODEL_NAME,\n",
    "    keep_alive=-1,\n",
    "    format=\"json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455038c-cc21-4de1-9062-1a4259f8eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa881d2d-50f2-4da1-a432-87593c832b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response_simple(request_str):\n",
    "    system_prompt = \"\"\n",
    "    \n",
    "    # Create a properly formatted prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        # (\"system\", system_prompt),\n",
    "        (\"human\", request_str)\n",
    "    ])\n",
    "    \n",
    "    # Format the final prompt before passing it to `llm.invoke()`\n",
    "    formatted_prompt = prompt.format_messages()\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Print the response\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed49ab1-692f-4c78-bf6f-c5dae2646178",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [\n",
    "    \"Какой рецепт вегетарианской пасты?\",\n",
    "    \"Как приготовить блинчики?\",\n",
    "    \"Что можно приготовить из куриного филе?\",\n",
    "    \"Как сделать шоколадный торт?\",\n",
    "    \"Как приготовить рис для суши?\",\n",
    "    \"Как приготовить суп-пюре из тыквы?\",\n",
    "    \"Что можно приготовить из картошки и сыра?\",\n",
    "    \"Как сделать домашний хлеб?\",\n",
    "    \"Какие десерты можно приготовить без муки?\",\n",
    "    \"Как приготовить рис с курицей и овощами?\",\n",
    "    \"Как сделать соус для пасты?\",\n",
    "    \"Как приготовить морковные котлеты?\",\n",
    "    \"Что приготовить на ужин за 30 минут?\",\n",
    "    \"Как сделать домашнюю пиццу?\",\n",
    "    \"Что можно приготовить из куриного филе, риса и брокколи?\",\n",
    "    \"Как приготовить сладкий омлет?\",\n",
    "    \"Как сделать домашнее мороженое без мороженицы?\",\n",
    "    \"Что можно приготовить для обеда, если нет мяса?\",\n",
    "    \"Как приготовить крем-суп из грибов?\",\n",
    "    \"Как приготовить рыбу с картошкой в духовке?\",\n",
    "    \"Что приготовить для завтрака, если есть яйца, авокадо и помидоры?\",\n",
    "    \"Как приготовить куриные крылышки на гриле?\",\n",
    "    \"Что приготовить на праздничный ужин?\",\n",
    "    \"Как приготовить соус бешамель?\",\n",
    "    \"Как сделать торт без яиц?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1077b-d29f-4e8f-b451-a96bc1988148",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for req in tqdm(requests):\n",
    "    start = time.time()\n",
    "    res = llm_response_simple(req)\n",
    "    responses.append({\n",
    "        \"request\": req,\n",
    "        \"time\": time.time() - start,\n",
    "        \"reqponse\": res,\n",
    "        \"desc\": \"llm_response_simple\"\n",
    "    })\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b5785-908a-4bbb-9e19-dd12ec2b0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4914e2-edc0-4485-a6de-e95b2ca39222",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_SIMPLE = \"Ты — кулинарный помощник\"\n",
    "SYSTEM_PROMPT = \"\"\"Ты — кулинарный помощник, который рекомендует рецепты на основе запросов пользователей. \"\n",
    "            \"Используй доступные ингредиенты и предпочтения пользователя, чтобы предложить лучший рецепт. \"\n",
    "            \"Если у пользователя нет конкретных предпочтений, предложи что-то популярное или сезонное. \"\n",
    "            \"Опиши рецепт кратко: укажи название, основные ингредиенты и способ приготовления. \"\n",
    "            \"Если возможно, укажи калорийность и полезные свойства блюда. \"\n",
    "            \"Отвечай только на русском языке.\"\"\"\n",
    "\n",
    "def llm_response_with_prompt(request_str, system_prompt):\n",
    "    \n",
    "    # Create a properly formatted prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", request_str)\n",
    "    ])\n",
    "    \n",
    "    # Format the final prompt before passing it to `llm.invoke()`\n",
    "    formatted_prompt = prompt.format_messages()\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Print the response\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8430aef-1b48-45d0-a1fc-3cba565f81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_simple_prompt = []\n",
    "for req in tqdm(requests):\n",
    "    start = time.time()\n",
    "    res = llm_response_with_prompt(req, SYSTEM_PROMPT_SIMPLE)\n",
    "    responses_simple_prompt.append({\n",
    "        \"request\": req,\n",
    "        \"time\": time.time() - start,\n",
    "        \"reqponse\": res,\n",
    "        \"desc\": \"llm_response_with_prompt(req, SYSTEM_PROMPT_SIMPLE)\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87e7b9-d90e-4398-b92c-28bc6061d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_prompt = []\n",
    "for req in tqdm(requests):\n",
    "    start = time.time()\n",
    "    res = llm_response_with_prompt(req, SYSTEM_PROMPT)\n",
    "    responses_prompt.append({\n",
    "        \"request\": req,\n",
    "        \"time\": time.time() - start,\n",
    "        \"reqponse\": res,\n",
    "        \"desc\": \"llm_response_with_prompt(req, SYSTEM_PROMPT)\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a484f-2473-4dcb-94ce-c51d6648a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"responses.json\", \"w\") as outfile: \n",
    "    json.dump(responses, outfile)\n",
    "\n",
    "with open(\"responses_prompt.json\", \"w\") as outfile: \n",
    "    json.dump(responses_prompt, outfile)\n",
    "\n",
    "with open(\"responses_simple_prompt.json\", \"w\") as outfile: \n",
    "    json.dump(responses_simple_prompt, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155001a2-4187-4e2d-9452-13eeeb9deedd",
   "metadata": {},
   "source": [
    "### Process documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f630bd2-e94a-4512-b694-c8974b1a74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('recipe_str.pickle', 'rb') as handle:\n",
    "    recipe_str = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc1645-c4ec-46a4-b43d-ef493fa5e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "recipe_str_small = random.sample(recipe_str, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ab42c-b3e5-438e-bff1-0f2817a5ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recipe_str_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa87a9f-b28b-4071-8edb-10a27f5486d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "def process_documents(docs_dir: str = \"documents\"):\n",
    "    # Initialize embeddings and text splitter\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # Process PDF files\n",
    "    pdf_files = [f for f in os.listdir(docs_dir) if f.endswith(\".pdf\")]\n",
    "    print(pdf_files)\n",
    "    if not pdf_files:\n",
    "        raise ValueError(f\"No PDF files found in {docs_dir}\")\n",
    "\n",
    "    all_docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(docs_dir, pdf_file)\n",
    "        print(file_path)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load()\n",
    "        \n",
    "        # Add metadata to each page\n",
    "        for page_num, page in enumerate(pages, start=1):\n",
    "            page.metadata.update({\n",
    "                \"source\": pdf_file,\n",
    "                \"page_number\": page_num,\n",
    "                \"chunk_id\": str(uuid.uuid4())[:8]\n",
    "            })\n",
    "\n",
    "        # Split pages into chunks\n",
    "        chunks = text_splitter.split_documents(pages)\n",
    "        all_docs.extend(chunks)\n",
    "\n",
    "    # Create/update vector store\n",
    "    Chroma.from_documents(\n",
    "        documents=all_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"chroma_db_example_1\",\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "        collection_name=\"main_collection\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd8632-409c-4d27-b649-52c77d6f716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = process_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e0f5e7-3c8b-4e3e-b375-c467f9bb07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "\n",
    "def get_text_chunks_langchain():\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    docs = []\n",
    "    for n, text in enumerate(recipe_str_small):\n",
    "        doc = Document(page_content=text)\n",
    "        doc.metadata.update({\n",
    "                \"chunk_id\": str(uuid.uuid4())[:8]\n",
    "            })\n",
    "        docs.append(doc)\n",
    "        # chunks = text_splitter.split_text([doc])\n",
    "        # docs.extend(chunks)\n",
    "    return docs\n",
    "\n",
    "\n",
    "docs = get_text_chunks_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1791fc7-0367-490d-9e5c-260004ddc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd295a1-fca0-40e0-bdda-75b885ae0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(docs)\n",
    "all_docs = []\n",
    "all_docs.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4d794-a5b1-4562-9984-cbe4a03f76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5319b-2e2a-42b7-9381-ac3c90b1c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "Chroma.from_documents(\n",
    "        documents=all_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"chroma_db_recipe\",\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "        collection_name=\"main_collection\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474f90c-1f4a-4264-b931-e46598bee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieve import DocumentRetriever\n",
    "import ollama\n",
    "import regex as re\n",
    "\n",
    "class QAPipeline:\n",
    "    def __init__(self):\n",
    "        self.retriever = DocumentRetriever()\n",
    "        \n",
    "    PROMPT_TEMPLATE = \"\"\"Context information:\n",
    "        {context}\n",
    "\n",
    "        Using the context above and your general knowledge, answer this question:\n",
    "        Question: {question}\n",
    "\n",
    "        Format requirements:\n",
    "\n",
    "        - If uncertain, say \"The documents don't specify\"\"\"\n",
    "\n",
    "    def parse_response(self, response: str) -> dict:\n",
    "        \"\"\"Extract thinking and answer components without <answer> tags\"\"\"\n",
    "        # Extract thinking process\n",
    "        think_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "        \n",
    "        # Get everything AFTER </think> as the answer\n",
    "        answer_start = response.find('</think>') + len('</think>')\n",
    "        answer = response[answer_start:].strip()\n",
    "        \n",
    "        return {\n",
    "            \"thinking\": think_match.group(1).strip() if think_match else \"\",\n",
    "            \"answer\": answer,\n",
    "            \"raw_response\": response\n",
    "        }\n",
    "\n",
    "    def generate_answer(self, question: str, k: int = 5) -> dict:\n",
    "        \"\"\"Full QA workflow with enhanced output\"\"\"\n",
    "        try:\n",
    "            # Retrieve documents\n",
    "            context_docs = self.retriever.query_documents(question, k=k)\n",
    "            \n",
    "            # Format context preserving full metadata\n",
    "            context_str = \"\\n\".join(\n",
    "                f\"[Document {idx+1}] {doc['source']} (Page {doc['page']}):\\n{doc['text']}\"\n",
    "                for idx, doc in enumerate(context_docs)\n",
    "            )\n",
    "            \n",
    "            # Generate response\n",
    "            response = ollama.generate(\n",
    "                model=\"deepseek-r1:latest\",\n",
    "                prompt=self.PROMPT_TEMPLATE.format(\n",
    "                    context=context_str,\n",
    "                    question=question\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Parse components\n",
    "            parsed = self.parse_response(response['response'])\n",
    "            \n",
    "            return {\n",
    "                **parsed,\n",
    "                \"sources\": [\n",
    "                    {\n",
    "                        # \"source\": doc[\"source\"],\n",
    "                        # \"page\": doc[\"page\"],\n",
    "                        \"confidence\": doc[\"score\"],\n",
    "                        # \"full_text\": doc[\"text\"]\n",
    "                    } for doc in context_docs\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"thinking\": \"\",\n",
    "                \"answer\": \"Failed to generate response\",\n",
    "                \"sources\": []\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbaea19-75e7-4672-a44d-847a8fa816df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_venv)",
   "language": "python",
   "name": "llm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
