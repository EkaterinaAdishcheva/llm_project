{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4377bac9-69d4-436c-b53f-b735ec9982c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running"
     ]
    }
   ],
   "source": [
    "!curl http://127.0.0.1:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3067c8d-7501-47d9-8b3d-2368ba47aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "MODEL_NAME =  \"mistral\"# \"deepseek-r1:7b\" # \"mistral\"  #Change to \"phi3\" or \"gemma:2b\" if needed\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = OllamaLLM(\n",
    "    model=MODEL_NAME,\n",
    "    keep_alive=-1,\n",
    "    format=\"json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e455038c-cc21-4de1-9062-1a4259f8eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa881d2d-50f2-4da1-a432-87593c832b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response_simple(request_str):\n",
    "    system_prompt = \"\"\n",
    "    \n",
    "    # Create a properly formatted prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        # (\"system\", system_prompt),\n",
    "        (\"human\", request_str)\n",
    "    ])\n",
    "    \n",
    "    # Format the final prompt before passing it to `llm.invoke()`\n",
    "    formatted_prompt = prompt.format_messages()\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Print the response\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ed49ab1-692f-4c78-bf6f-c5dae2646178",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [\n",
    "    \"Какой рецепт вегетарианской пасты?\",\n",
    "    \"Как приготовить блинчики?\",\n",
    "    \"Что можно приготовить из куриного филе?\",\n",
    "    \"Как сделать шоколадный торт?\",\n",
    "    \"Как приготовить рис для суши?\",\n",
    "    \"Как приготовить суп-пюре из тыквы?\",\n",
    "    \"Что можно приготовить из картошки и сыра?\",\n",
    "    \"Как сделать домашний хлеб?\",\n",
    "    \"Какие десерты можно приготовить без муки?\",\n",
    "    \"Как приготовить рис с курицей и овощами?\",\n",
    "    \"Как сделать соус для пасты?\",\n",
    "    \"Как приготовить морковные котлеты?\",\n",
    "    \"Что приготовить на ужин за 30 минут?\",\n",
    "    \"Как сделать домашнюю пиццу?\",\n",
    "    \"Что можно приготовить из куриного филе, риса и брокколи?\",\n",
    "    \"Как приготовить сладкий омлет?\",\n",
    "    \"Как сделать домашнее мороженое без мороженицы?\",\n",
    "    \"Что можно приготовить для обеда, если нет мяса?\",\n",
    "    \"Как приготовить крем-суп из грибов?\",\n",
    "    \"Как приготовить рыбу с картошкой в духовке?\",\n",
    "    \"Что приготовить для завтрака, если есть яйца, авокадо и помидоры?\",\n",
    "    \"Как приготовить куриные крылышки на гриле?\",\n",
    "    \"Что приготовить на праздничный ужин?\",\n",
    "    \"Как приготовить соус бешамель?\",\n",
    "    \"Как сделать торт без яиц?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efd55dda-7cd4-4715-bd85-fe0b5d7f635c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\\n\"Mmm, салат - это очень увлекательное дело! Я смогу предложить тебе несколько идеи для приготовления салата. Посмотрим на существующий состав и подумаем, чем его можно дополнить:) Что мы имеем? Картофель или огурцы? Листья салата или свежее фрукты? А что ты предпочитаешь? Может, светлый или более густой салат? Спасибо за вопрос! Здесь некоторые идеи для добавления в твой салат:) 1. Оливки, огурцы в маринаде, лук репчатый или спагетти; 2. Горчица, красные перец чилий, солонка и авокадо; 3. Листья шпинат, томат, сыр моцарелла и кубики салами; 4. Яблоко, апельсин или фундук, маслины, орехи и брусники; 5. Свежие лук и цилиндрический перец, сметана и сухарик для мяса; 6. Огурцы, морской зубчатка, сельдерея, томат и сыр пекора. Кажется, теперь ты можешь создать совершенно уникальный салат! :) }   { \"\\n\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response_simple(\"Что добавить в салат?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6f1077b-d29f-4e8f-b451-a96bc1988148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:21<00:56,  3.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m req \u001b[38;5;129;01min\u001b[39;00m tqdm(requests):\n\u001b[1;32m      3\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mllm_response_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m: req,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m: time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreqponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: res,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_response_simple\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m     })\n",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m, in \u001b[0;36mllm_response_simple\u001b[0;34m(request_str)\u001b[0m\n\u001b[1;32m     11\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat_messages()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Get response from LLM\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the response\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/workspace/DeepSeek-RAG-Chatbot/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:390\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    388\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    402\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/DeepSeek-RAG-Chatbot/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:763\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    757\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    761\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    762\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/DeepSeek-RAG-Chatbot/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:966\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    953\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    964\u001b[0m         )\n\u001b[1;32m    965\u001b[0m     ]\n\u001b[0;32m--> 966\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/workspace/DeepSeek-RAG-Chatbot/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    779\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 787\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    791\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    796\u001b[0m         )\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    798\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/workspace/DeepSeek-RAG-Chatbot/venv/lib/python3.10/site-packages/langchain_ollama/llms.py:288\u001b[0m, in \u001b[0;36mOllamaLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 288\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/workspace/DeepSeek-RAG-Chatbot/venv/lib/python3.10/site-packages/langchain_ollama/llms.py:256\u001b[0m, in \u001b[0;36mOllamaLLM._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    249\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    254\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    255\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    258\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[1;32m    259\u001b[0m                 text\u001b[38;5;241m=\u001b[39mstream_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    260\u001b[0m                 generation_info\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    261\u001b[0m                     \u001b[38;5;28mdict\u001b[39m(stream_resp) \u001b[38;5;28;01mif\u001b[39;00m stream_resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    262\u001b[0m                 ),\n\u001b[1;32m    263\u001b[0m             )\n",
      "File \u001b[0;32m/workspace/DeepSeek-RAG-Chatbot/venv/lib/python3.10/site-packages/langchain_ollama/llms.py:211\u001b[0m, in \u001b[0;36mOllamaLLM._create_generate_stream\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    208\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    210\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_params(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/DeepSeek-RAG-Chatbot/venv/lib/python3.10/site-packages/ollama/_client.py:171\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[0;32m--> 171\u001b[0m   part \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m:=\u001b[39m part\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(err)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:338\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m--> 338\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for req in tqdm(requests):\n",
    "    start = time.time()\n",
    "    res = llm_response_simple(req)\n",
    "    responses.append({\n",
    "        \"request\": req,\n",
    "        \"time\": time.time() - start,\n",
    "        \"reqponse\": res,\n",
    "        \"desc\": \"llm_response_simple\"\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "374b5785-908a-4bbb-9e19-dd12ec2b0a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'request': 'Какой рецепт вегетарианской пасты?',\n",
       "  'time': 0.730560302734375,\n",
       "  'reqponse': '{}\\n\\n\\n    \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n     \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \\n   \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  ',\n",
       "  'desc': 'llm_response_simple'},\n",
       " {'request': 'Как приготовить блинчики?',\n",
       "  'time': 14.17859673500061,\n",
       "  'reqponse': '{ \"components\": [ { \"id\": 1, \"name\": \"面粉\", \"category\": \" dry ingredients\", \"amount\": \"200 grams\" }, { \"id\": 2, \"name\": \"水\", \"category\": \"liquid\", \"amount\": \"300 ml\" }, { \"id\": 3, \"name\": \"糖浆\", \"category\": \"sweeteners\", \"amount\": \"5 tablespoons\" }, { \"id\": 4, \"name\": \"盐\", \"category\": \" seasonings\", \"amount\": \" teaspoon\" }, { \"id\": 5, \"name\": \"鸡蛋\", \"category\": \"proteins\", \"amount\": \"2 large\" } ], \"steps\": [ { \"step\": \"1\", \"action\": \"Mix the sifted面粉 in a bowl with the appropriate amount of 水.\", \"description\": \"将过筛的面粉与适量的水混合。\" }, { \"step\": \"2\", \"action\": \"Stir gently until a thin dough forms.\", \"description\": \"用勺子轻柔地搅拌，直到形成薄而均匀的面团。\", \"steps\": [ { \"substep\": \"1\", \"action\": \"Gently stir with a spatula.\", \"description\": \"用铁锅铲轻轻翻动。\" }, { \"substep\": 2, \"action\": \"Continue mixing until the dough comes together.\", \"description\": \"继续搅拌，直到面团团成一体。\" } ]}, { \"step\": 3, \"action\": \"Heat a non-stick skillet over medium heat.\", \"description\": \"使用平底锅在中火加热，确保不沾涂层。\", \"steps\": [ { \"substep\": 1, \"action\": \"Pour in the sugar浆 and stir until it dissolves.\", \"description\": \"倒入糖浆并搅拌使其溶解。\", \"substeps\": [ { \"subsubstep\": 1, \"action\": \"Ensure the pan is clean and dry before pouring.\", \"description\": \"在倒糖浆前确保平底锅干净且无水分。\" } ]}, { \"substep\": 2, \"action\": \"Mix well to incorporate all ingredients.\", \"description\": \"搅拌均匀，使所有成分充分结合。\", \"substeps\": [ { \"subsubstep\": 1, \"action\": \"Gradually add the eggs one at a time while mixing.\", \"description\": \"每次加入一个鸡蛋并不断搅拌。\" }, { \"subsubstep\": 2, \"action\": \"Continuously stir until well combined.\", \"description\": \"持续搅拌，直到完全融合。\" } ]} ], \"step\": 4, \"action\": \"Pour the dough onto a floured surface and shape into small balls.\", \"description\": \"将面团放在撒了面粉的平面上，揉成小球。\", \"steps\": [ { \"substep\": 1, \"action\": \"Divide the dough into equal portions.\", \"description\": \"将面团分成相等的部分。\", \"steps\": [ { \"subsubstep\": 1, \"action\": \"Roll out each portion into a small ball with your hands.\", \"description\": \"用双手揉成小球。\" }, { \"subsubstep\": 2, \"action\": \"Knead gently for about 5-7 minutes to shape properly.\", \"description\": \"在平底锅上轻轻地揉约5到7分钟，使其形状均匀。\" } ]}, { \"substep\": 2, \"action\": \"Heat the skillet again and pour in a bit of 水 or milk if needed.\", \"description\": \"重新加热平底锅，加入少量的水或牛奶以调整粘度。\", \"steps\": [ { \"subsubstep\": 1, \"action\": \"Mix well to incorporate the liquid.\", \"description\": \"搅拌均匀，使液体充分融入面团。\" }, { \"subsubstep\": 2, \"action\": \"Knead gently again for about 5-7 minutes until smooth and elastic.\", \"description\": \"继续轻柔揉约5到7分钟，使其光滑且有弹性。\" } ]} ], \"step\": 5, \"action\": \"Wrap each dough ball in plastic wrap and let rise for about 1 hour or until doubled in size.\", \"description\": \"将每个面团包裹在塑料薄膜上，让它发酵大约1小时或直到体积翻倍。\", \"steps\": [ { \"substep\": 1, \"action\": \"Let it rise in a warm place to activate the yeast if using active dry yeast.\", \"description\": \"如果使用的是活性干酵母，则将其置于温暖的地方以使其发酵。\" }, { \"substep\": 2, \"action\": \"During this time, prepare the toppings and assemble the buns accordingly.\", \"description\": \"在这段时间内，准备装饰物并按照计划制作煎饼。\", \"steps\": [ { \"subsubstep\": 1, \"action\": \"Preheat the oven to 200°C (400°F).\", \"description\": \"预热烤箱至200摄氏度（400华氏度）。\" }, { \"subsubstep\": 2, \"action\": \"Brush the top of each dough ball with melted butter or oil before placing them in the oven.\", \"description\": \"在将面团放入烤箱前，用黄油或油涂抹每一片面团的顶部。\" } ]} ]}, { \"step\": 6, \"action\": \"Remove the dough balls from the plastic wrap and place them on a baking tray lined with parchment paper or silicon mats.\", \"description\": \"取出发酵好的面团，并将其放置在涂有牛皮纸或Silicon胶垫的烤盘上。\", \"steps\": [ { \"substep\": 1, \"action\": \"Ensure there is enough space so they won\\'t overlap.\", \"description\": \"确保它们不会堆叠在一起。\" } ]}, { \"step\": 7, \"action\": \" Bake in the preheated oven for about 15-20 minutes, or until golden brown.\", \"description\": \"在预热好的烤箱中烘烤约15到20分钟，直到表面金黄色。\", \"steps\": [ { \"subsubstep\": 1, \"action\": \"Check a few dough balls to ensure even cooking and remove them from the oven when they reach the desired doneness.\", \"description\": \"检查几片面团，确保均匀烘烤，并在达到所需成熟度时取出。\" } ]}, { \"step\": 8, \"action\": \"Let the buns rest for about 5 minutes after taking them out of the oven to cool slightly.\", \"description\": \"从烤箱取出后，让煎饼稍微冷却约5分钟以使其稍凉。\", \"steps\": [ { \"substep\": 1, \"action\": \" optional:Brush with melted butter or oil again for additional moisture.\", \"description\": \"可选：再次涂抹黄油或油以保持湿度。\" } ]}, { \"step\": 9, \"action\": \"Serve warm with your favorite toppings.\", \"description\": \"趁热食用，搭配你喜欢的配料。\", \"steps\": [ { \"subsubstep\": 1, \"action\": \"Enjoy the fresh出炉的煎饼吧！\", \"description\": \"享受 freshly baked buns today!.\" } ]} ]} \\n\\t\\t\\t\\t                \\n\\n\\t\\t\\t\\t                \\n\\n\\t\\t\\t\\t                \\n\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t                \\n\\n\\t\\t\\t\\t                \\n\\n\\t\\t\\t\\t                \\n\\t\\t\\t\\t                \\n\\n\\t\\t\\t\\t                \\n\\t\\t\\t\\t\\n\\n',\n",
       "  'desc': 'llm_response_simple'},\n",
       " {'request': 'Что можно приготовить из куриного филе?',\n",
       "  'time': 0.6483151912689209,\n",
       "  'reqponse': '{}\\n  \\n   \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n\\n\\n\\n\\n  \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n  \\n\\n',\n",
       "  'desc': 'llm_response_simple'},\n",
       " {'request': 'Как сделать шоколадный торт?',\n",
       "  'time': 0.39995360374450684,\n",
       "  'reqponse': '{}\\n\\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             \\n\\n             ',\n",
       "  'desc': 'llm_response_simple'},\n",
       " {'request': 'Как приготовить рис для суши?',\n",
       "  'time': 2.8910601139068604,\n",
       "  'reqponse': '{ \"items\": [ { \"id\": 0, \"uri\": \"/answers/45678901234567890123.jpg\", \"content\": \"Step-by-step explanation and answer: To dry rice completely and evenly, follow these steps:\\\\n\\\\n1. **Wash the rice:** Start by washing the raw rice thoroughly under cold water to remove any foreign matter or stains.\\\\n\\\\n2. **Dry in the sun:** Place the washed rice in an open container outside in direct sunlight for a few hours to speed up the drying process.\\\\n\\\\n3. **Flatten and stack:** Once some of the moisture is removed, flatten the rice into thin layers and place them on a baking sheet or a shallow tray. Make sure they are not too tightly packed so that they can dry out evenly.\\\\n\\\\n4. **Bake in an oven:** Preheat the oven to 180°C (350°F). Place the flattened rice directly in the oven and bake for about 20-25 minutes, until it is completely dry.\\\\n\\\\n5. **Store properly:** After drying, place the baked rice in an airtight container to preserve its shape and keep it fresh.\\\\n\" } ] }\\n  \\n  \\n      \\n  \\n      \\n  \\n      \\n  \\n      \\n  \\n      \\n  \\n      \\n  \\n      \\n  \\n      \\n  \\n      \\n  \\n      \\n',\n",
       "  'desc': 'llm_response_simple'},\n",
       " {'request': 'Как приготовить суп-пюре из тыквы?',\n",
       "  'time': 0.9249558448791504,\n",
       "  'reqponse': '{ \"response\": \"Добро пожаловать! Пожалуй, что вы хотите сделать today. Если это не то, что ищите, уточните, пожалуйста, wording of your request.\" }\\n\\n\\n\\n\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n',\n",
       "  'desc': 'llm_response_simple'},\n",
       " {'request': 'Что можно приготовить из картошки и сыра?',\n",
       "  'time': 0.6733253002166748,\n",
       "  'reqponse': '{}\\n \\n         \\n       \\n\\n       \\n\\n       \\n\\n       \\n\\n       \\n\\n      \\n\\n   \\n\\n   \\n\\n\\n   \\n\\n\\n\\n   \\n\\n\\n\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "  'desc': 'llm_response_simple'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4914e2-edc0-4485-a6de-e95b2ca39222",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_SIMPLE = \"Ты — кулинарный помощник\"\n",
    "SYSTEM_PROMPT = \"\"\"Ты — кулинарный помощник, который рекомендует рецепты на основе запросов пользователей. \"\n",
    "            \"Используй доступные ингредиенты и предпочтения пользователя, чтобы предложить лучший рецепт. \"\n",
    "            \"Если у пользователя нет конкретных предпочтений, предложи что-то популярное или сезонное. \"\n",
    "            \"Опиши рецепт кратко: укажи название, основные ингредиенты и способ приготовления. \"\n",
    "            \"Если возможно, укажи калорийность и полезные свойства блюда. \"\n",
    "            \"Отвечай только на русском языке.\"\"\"\n",
    "\n",
    "def llm_response_with_prompt(request_str, system_prompt):\n",
    "    \n",
    "    # Create a properly formatted prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", request_str)\n",
    "    ])\n",
    "    \n",
    "    # Format the final prompt before passing it to `llm.invoke()`\n",
    "    formatted_prompt = prompt.format_messages()\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Print the response\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8430aef-1b48-45d0-a1fc-3cba565f81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_simple_prompt = []\n",
    "for req in tqdm(requests):\n",
    "    start = time.time()\n",
    "    res = llm_response_with_prompt(req, SYSTEM_PROMPT_SIMPLE)\n",
    "    responses_simple_prompt.append({\n",
    "        \"request\": req,\n",
    "        \"time\": time.time() - start,\n",
    "        \"reqponse\": res,\n",
    "        \"desc\": \"llm_response_with_prompt(req, SYSTEM_PROMPT_SIMPLE)\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87e7b9-d90e-4398-b92c-28bc6061d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_prompt = []\n",
    "for req in tqdm(requests):\n",
    "    start = time.time()\n",
    "    res = llm_response_with_prompt(req, SYSTEM_PROMPT)\n",
    "    responses_prompt.append({\n",
    "        \"request\": req,\n",
    "        \"time\": time.time() - start,\n",
    "        \"reqponse\": res,\n",
    "        \"desc\": \"llm_response_with_prompt(req, SYSTEM_PROMPT)\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a484f-2473-4dcb-94ce-c51d6648a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"responses.json\", \"w\") as outfile: \n",
    "    json.dump(responses, outfile)\n",
    "\n",
    "with open(\"responses_prompt.json\", \"w\") as outfile: \n",
    "    json.dump(responses_prompt, outfile)\n",
    "\n",
    "with open(\"responses_simple_prompt.json\", \"w\") as outfile: \n",
    "    json.dump(responses_simple_prompt, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155001a2-4187-4e2d-9452-13eeeb9deedd",
   "metadata": {},
   "source": [
    "### Process documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f630bd2-e94a-4512-b694-c8974b1a74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('recipe_str.pickle', 'rb') as handle:\n",
    "    recipe_str = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc1645-c4ec-46a4-b43d-ef493fa5e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "recipe_str_small = random.sample(recipe_str, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ab42c-b3e5-438e-bff1-0f2817a5ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recipe_str_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa87a9f-b28b-4071-8edb-10a27f5486d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "def process_documents(docs_dir: str = \"documents\"):\n",
    "    # Initialize embeddings and text splitter\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # Process PDF files\n",
    "    pdf_files = [f for f in os.listdir(docs_dir) if f.endswith(\".pdf\")]\n",
    "    print(pdf_files)\n",
    "    if not pdf_files:\n",
    "        raise ValueError(f\"No PDF files found in {docs_dir}\")\n",
    "\n",
    "    all_docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(docs_dir, pdf_file)\n",
    "        print(file_path)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load()\n",
    "        \n",
    "        # Add metadata to each page\n",
    "        for page_num, page in enumerate(pages, start=1):\n",
    "            page.metadata.update({\n",
    "                \"source\": pdf_file,\n",
    "                \"page_number\": page_num,\n",
    "                \"chunk_id\": str(uuid.uuid4())[:8]\n",
    "            })\n",
    "\n",
    "        # Split pages into chunks\n",
    "        chunks = text_splitter.split_documents(pages)\n",
    "        all_docs.extend(chunks)\n",
    "\n",
    "    # Create/update vector store\n",
    "    Chroma.from_documents(\n",
    "        documents=all_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"chroma_db_example_1\",\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "        collection_name=\"main_collection\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd8632-409c-4d27-b649-52c77d6f716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = process_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e0f5e7-3c8b-4e3e-b375-c467f9bb07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "\n",
    "def get_text_chunks_langchain():\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    docs = []\n",
    "    for n, text in enumerate(recipe_str_small):\n",
    "        doc = Document(page_content=text)\n",
    "        doc.metadata.update({\n",
    "                \"chunk_id\": str(uuid.uuid4())[:8]\n",
    "            })\n",
    "        docs.append(doc)\n",
    "        # chunks = text_splitter.split_text([doc])\n",
    "        # docs.extend(chunks)\n",
    "    return docs\n",
    "\n",
    "\n",
    "docs = get_text_chunks_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1791fc7-0367-490d-9e5c-260004ddc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd295a1-fca0-40e0-bdda-75b885ae0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(docs)\n",
    "all_docs = []\n",
    "all_docs.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4d794-a5b1-4562-9984-cbe4a03f76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5319b-2e2a-42b7-9381-ac3c90b1c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "Chroma.from_documents(\n",
    "        documents=all_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"chroma_db_recipe\",\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "        collection_name=\"main_collection\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474f90c-1f4a-4264-b931-e46598bee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieve import DocumentRetriever\n",
    "import ollama\n",
    "import regex as re\n",
    "\n",
    "class QAPipeline:\n",
    "    def __init__(self):\n",
    "        self.retriever = DocumentRetriever()\n",
    "        \n",
    "    PROMPT_TEMPLATE = \"\"\"Context information:\n",
    "        {context}\n",
    "\n",
    "        Using the context above and your general knowledge, answer this question:\n",
    "        Question: {question}\n",
    "\n",
    "        Format requirements:\n",
    "\n",
    "        - If uncertain, say \"The documents don't specify\"\"\"\n",
    "\n",
    "    def parse_response(self, response: str) -> dict:\n",
    "        \"\"\"Extract thinking and answer components without <answer> tags\"\"\"\n",
    "        # Extract thinking process\n",
    "        think_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "        \n",
    "        # Get everything AFTER </think> as the answer\n",
    "        answer_start = response.find('</think>') + len('</think>')\n",
    "        answer = response[answer_start:].strip()\n",
    "        \n",
    "        return {\n",
    "            \"thinking\": think_match.group(1).strip() if think_match else \"\",\n",
    "            \"answer\": answer,\n",
    "            \"raw_response\": response\n",
    "        }\n",
    "\n",
    "    def generate_answer(self, question: str, k: int = 5) -> dict:\n",
    "        \"\"\"Full QA workflow with enhanced output\"\"\"\n",
    "        try:\n",
    "            # Retrieve documents\n",
    "            context_docs = self.retriever.query_documents(question, k=k)\n",
    "            \n",
    "            # Format context preserving full metadata\n",
    "            context_str = \"\\n\".join(\n",
    "                f\"[Document {idx+1}] {doc['source']} (Page {doc['page']}):\\n{doc['text']}\"\n",
    "                for idx, doc in enumerate(context_docs)\n",
    "            )\n",
    "            \n",
    "            # Generate response\n",
    "            response = ollama.generate(\n",
    "                model=\"deepseek-r1:latest\",\n",
    "                prompt=self.PROMPT_TEMPLATE.format(\n",
    "                    context=context_str,\n",
    "                    question=question\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Parse components\n",
    "            parsed = self.parse_response(response['response'])\n",
    "            \n",
    "            return {\n",
    "                **parsed,\n",
    "                \"sources\": [\n",
    "                    {\n",
    "                        # \"source\": doc[\"source\"],\n",
    "                        # \"page\": doc[\"page\"],\n",
    "                        \"confidence\": doc[\"score\"],\n",
    "                        # \"full_text\": doc[\"text\"]\n",
    "                    } for doc in context_docs\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"thinking\": \"\",\n",
    "                \"answer\": \"Failed to generate response\",\n",
    "                \"sources\": []\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbaea19-75e7-4672-a44d-847a8fa816df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_venv)",
   "language": "python",
   "name": "llm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
