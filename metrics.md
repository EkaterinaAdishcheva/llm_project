**1. Response Accuracy**
Definition: Measures how accurate and correct the response is in relation to the question asked, especially for specific details like ingredients, cooking methods, or timing.
Metric: Accuracy Score ‚Äî percentage of correct responses based on a predefined correct answer or a set of expert responses.

**2. Relevance**
Definition: Assesses how well the response answers the user‚Äôs query and whether it aligns with the user's intent (e.g., specific recipe, dietary preferences).
Metric: Relevance Score ‚Äî based on how directly the answer addresses the user's question. Can be scored on a Likert scale (e.g., 1-5) by human evaluators or through an automated metric that looks for key terms in the response.
Formula: Average relevance score from user feedback or evaluator rating.

**3. Clarity and Coherence**
Definition: Measures how clear and easy to understand the response is. A response should be well-structured and free from confusion.
Metric: Clarity Score ‚Äî scored by human evaluators or using automated NLP metrics (e.g., BLEU, ROUGE).
Formula: Average score from human evaluators (1-5 scale) or NLP-based coherence evaluation.

**4. Response Time**
Definition: Measures the time it takes for the system to generate a response after receiving a query.
Metric: Average Response Time ‚Äî the time (in seconds or milliseconds) it takes for the system to respond.

**5. User Satisfaction**
Definition: Measures how satisfied users are with the responses they receive, especially in terms of usefulness and quality.
Metric: User Satisfaction Score ‚Äî typically measured using surveys or feedback forms. Can use a Likert scale (1‚Äì5) to rate overall satisfaction.
Formula: Average user satisfaction score from feedback or survey responses.

**6. Consistency**
Definition: Assesses whether the system provides consistent answers to the same or similar queries over time.
Metric: Consistency Score ‚Äî measures how similar responses are when the same question is asked multiple times. Can be evaluated through a comparison of answers (e.g., semantic similarity).
Formula: Percentage of identical or similar responses across multiple queries.
Method: Use semantic similarity metrics like cosine similarity between vectors from BERT or another transformer-based model.

**7. Novelty and Diversity (in the context of recipes)**
Definition: Evaluates how diverse or unique the recipe suggestions are. A good cooking advisor should not repeatedly suggest the same recipe unless it's a common choice.
Metric: Novelty Score ‚Äî measures how often the same or similar recipes are suggested.
Formula: Percentage of novel recipes suggested based on a reference set.
Method: Track if the same recipe is repeated for similar queries or based on ingredient constraints.

**8. Handling of Uncommon Queries**
Definition: Assesses the system‚Äôs ability to handle rare or difficult cooking-related queries (e.g., specific dietary needs, rare ingredients).
Metric: Success Rate for Uncommon Queries ‚Äî measures how often the system can provide a meaningful answer to uncommon or niche queries.

**9. Engagement or Interaction Quality**
Definition: Measures how well the system engages with the user, keeping them interested and encouraging further interaction (e.g., suggesting related recipes, asking follow-up questions).
Metric: Engagement Score ‚Äî can be evaluated through user feedback, e.g., "Would you like to try another recipe?" or based on follow-up questions.
Formula: Percentage of users who ask more than one follow-up question or provide positive engagement feedback.



üìå 1. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ (NLP-–º–µ—Ç—Ä–∏–∫–∏)

üîπ **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) ‚Äì —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å–ª–æ–≤ –º–µ–∂–¥—É –æ—Ç–≤–µ—Ç–∞–º–∏ –º–æ–¥–µ–ª–∏ –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏.

üîπ **BLEU** (Bilingual Evaluation Understudy) ‚Äì –∏–∑–º–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–æ–≤ (–ø–æ–ª–µ–∑–Ω–æ, –µ—Å–ª–∏ –µ—Å—Ç—å —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã).

üîπ **METEOR** (Metric for Evaluation of Translation with Explicit ORdering) ‚Äì —É–ª—É—á—à–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç BLEU, —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤.

üîπ **BERTScore** ‚Äì —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –ª—É—á—à–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–º—ã—Å–ª.


‚úÖ –í—ã–±–æ—Ä: –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã ‚Üí –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ROUGE + METEOR. –ï—Å–ª–∏ –Ω–µ—Ç ‚Äì BERTScore.


üìå 2. –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (RAG-–º–µ—Ç—Ä–∏–∫–∏)
üîπ Retrieval Hit Rate (RHR) ‚Äì –ø—Ä–æ—Ü–µ–Ω—Ç —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑–µ.
üîπ Mean Reciprocal Rank (MRR) ‚Äì —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤ —Å–ø–∏—Å–∫–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ).
üîπ Normalized Discounted Cumulative Gain (NDCG) ‚Äì –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –ø–æ–ª–µ–∑–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.

‚úÖ –í—ã–±–æ—Ä: –î–ª—è RAG –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç ‚Äì MRR + NDCG.

üìå 3. –û—Ü–µ–Ω–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞
üîπ Response Time (RT) ‚Äì –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ (–≤–∞–∂–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã).
üîπ Human Evaluation ‚Äì —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ —à–∫–∞–ª–µ 1-5).
üîπ Consistency Score ‚Äì –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ –Ω–∞ —Å—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã.

‚úÖ –í—ã–±–æ—Ä: –ï—Å–ª–∏ –≤–∞–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å ‚Äì Response Time. –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ‚Äì Human Evaluation.
